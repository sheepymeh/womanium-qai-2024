{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - MNIST Dataset\n",
    "\n",
    "In this notebook, we present our solution to task 3, where we train a QML model on the MNIST dataset. We make the following improvements to the tutorial presented\n",
    "\n",
    "* Testing on the Fashion MNIST dataset\n",
    "* Allow the QNN parameters to be trained\n",
    "* Use JAX instead of Keras\n",
    "* Use `jax.vmap` to speed up the QNN\n",
    "* Add augmentations to the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import grain.python as pygrain\n",
    "import dm_pix\n",
    "import orbax.checkpoint\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import requests\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from itertools import combinations\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "mnist = False\n",
    "qnn_wires = 9\n",
    "qnn_layers = 4\n",
    "\n",
    "assert qnn_wires == int(qnn_wires**0.5)**2\n",
    "\n",
    "SAVE_PATH = \"data/\"\n",
    "np.random.seed(0)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use the [Google Grain](https://github.com/google/grain) library to load our dataset. We use rotational augmentations to improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, fname: str, chunk_size=1024):\n",
    "\tif os.path.exists(fname):\n",
    "\t\treturn\n",
    "\n",
    "\tresp = requests.get(url, stream=True)\n",
    "\ttotal = int(resp.headers.get('content-length', 0))\n",
    "\twith open(fname, 'wb') as file, tqdm(\n",
    "\t\tdesc=fname,\n",
    "\t\ttotal=total,\n",
    "\t\tunit='iB',\n",
    "\t\tunit_scale=True,\n",
    "\t\tunit_divisor=1024,\n",
    "\t) as bar:\n",
    "\t\tfor data in resp.iter_content(chunk_size=chunk_size):\n",
    "\t\t\tsize = file.write(data)\n",
    "\t\t\tbar.update(size)\n",
    "\n",
    "def load_mnist(path):\n",
    "\tdownload('https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz', os.path.join(path, 'mnist.npz'))\n",
    "\treturn np.load(os.path.join(path, 'mnist.npz'))\n",
    "\n",
    "def load_fashion_mnist(path):\n",
    "\tbase = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n",
    "\tfiles = [\n",
    "\t\t\"train-labels-idx1-ubyte.gz\",\n",
    "\t\t\"train-images-idx3-ubyte.gz\",\n",
    "\t\t\"t10k-labels-idx1-ubyte.gz\",\n",
    "\t\t\"t10k-images-idx3-ubyte.gz\",\n",
    "\t]\n",
    "\n",
    "\tdef open_gzip(fname):\n",
    "\t\treturn gzip.open(os.path.join(path, fname), 'rb')\n",
    "\n",
    "\tfor fname in files:\n",
    "\t\tdownload(f'{base}{fname}', os.path.join(path, fname))\n",
    "\n",
    "\twith open_gzip(files[0]) as lbpath:\n",
    "\t\ty_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "\n",
    "\twith open_gzip(files[1]) as imgpath:\n",
    "\t\tx_train = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n",
    "\t\t\tlen(y_train), 28, 28\n",
    "\t\t)\n",
    "\n",
    "\twith open_gzip(files[2]) as lbpath:\n",
    "\t\ty_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "\n",
    "\twith open_gzip(files[3]) as imgpath:\n",
    "\t\tx_test = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n",
    "\t\t\tlen(y_test), 28, 28\n",
    "\t\t)\n",
    "\n",
    "\treturn {\n",
    "\t\t'x_train': x_train,\n",
    "\t\t'y_train': y_train,\n",
    "\t\t'x_test': x_test,\n",
    "\t\t'y_test': y_test,\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_fn, mean, std = (load_mnist, 0.1307, 0.3081) if mnist else (load_fashion_mnist, 0.2860, 0.3530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataSource(pygrain.RandomAccessDataSource[tuple[np.ndarray, np.ndarray]]):\n",
    "\tdef __init__(self, path, split):\n",
    "\t\tdata = load_fn(path)\n",
    "\t\tself.images = data[f'x_{split}'][..., np.newaxis]\n",
    "\t\tself.labels = data[f'y_{split}']\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.images)\n",
    "\n",
    "\tdef __getitem__(self, idx) -> tuple[np.ndarray, np.ndarray]:\n",
    "\t\timage, label = self.images[idx], self.labels[idx]\n",
    "\t\treturn image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform(pygrain.RandomMapTransform):\n",
    "\tdef __init__(self, mean, var, augment, flip_x=False, flip_y=False, rotate=0):\n",
    "\t\tself.augment = augment\n",
    "\t\tself.flip_x = flip_x\n",
    "\t\tself.flip_y = flip_y\n",
    "\t\tself.rotate = rotate\n",
    "\t\tself.mean = jnp.array(mean)\n",
    "\t\tself.var = jnp.array(var)\n",
    "\n",
    "\t# @jax.jit\n",
    "\tdef random_map(self, data: tuple[np.ndarray, np.ndarray], rng: np.random.Generator) -> tuple[jax.Array, jax.Array]:\n",
    "\t\timages, labels = data\n",
    "\t\timages, labels = jnp.array(images), jnp.array(labels)\n",
    "\n",
    "\t\tif self.augment:\n",
    "\t\t\tkey = jax.random.PRNGKey(rng.integers(0, 2**32))\n",
    "\n",
    "\t\t\tif self.flip_x:\n",
    "\t\t\t\timages = dm_pix.random_flip_up_down(key, images)\n",
    "\n",
    "\t\t\tif self.flip_y:\n",
    "\t\t\t\timages = dm_pix.random_flip_left_right(key, images)\n",
    "\n",
    "\t\t\tif self.rotate:\n",
    "\t\t\t\tangle = jax.random.uniform(key, shape=images.shape[0], minval=-self.rotate, maxval=self.rotate) / 180 * jnp.pi\n",
    "\t\t\t\timages = jax.vmap(\n",
    "\t\t\t\t\tpartial(dm_pix.rotate, mode='constant', cval=255),\n",
    "\t\t\t\t\tin_axes=[0, 0], out_axes=0\n",
    "\t\t\t\t)(images, angle)\n",
    "\n",
    "\t\timages /= 255\n",
    "\t\timages = jax.nn.standardize(\n",
    "\t\t\timages,\n",
    "\t\t\tmean=self.mean,\n",
    "\t\t\tvariance=self.var,\n",
    "\t\t\taxis=(2, 3)\n",
    "\t\t)\n",
    "\t\treturn images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing data loaders\n",
    "\n",
    "We use the known mean and std values for the MNIST dataset. However, we use the variance instead of the std as that is the expected input for `dm-pix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataSource(SAVE_PATH, 'train')\n",
    "train_sampler_fn = partial(\n",
    "\tpygrain.IndexSampler,\n",
    "\tnum_records=len(train_dataset),\n",
    "\tnum_epochs=1,\n",
    "\tshard_options=pygrain.NoSharding(),\n",
    "\tshuffle=True,\n",
    ")\n",
    "train_loader_fn = partial(\n",
    "\tpygrain.DataLoader,\n",
    "\tdata_source=train_dataset,\n",
    "\toperations=[\n",
    "\t\tpygrain.Batch(batch_size=batch_size, drop_remainder=False),\n",
    "\t\tImageTransform(mean, std ** 2, True, True, False, 10),\n",
    "\t],\n",
    "\tworker_count=2,\n",
    ")\n",
    "train_steps_per_epoch = len(train_dataset) // batch_size + 1\n",
    "\n",
    "test_dataset = ImageDataSource(SAVE_PATH, 'test')\n",
    "test_sampler = pygrain.IndexSampler(\n",
    "\tnum_records=len(test_dataset),\n",
    "\tnum_epochs=1,\n",
    "\tshard_options=pygrain.NoSharding(),\n",
    "\tshuffle=False,\n",
    "\tseed=0,\n",
    ")\n",
    "test_loader = pygrain.DataLoader(\n",
    "\tdata_source=test_dataset,\n",
    "\toperations=[\n",
    "\t\tpygrain.Batch(batch_size=batch_size, drop_remainder=False),\n",
    "\t\tImageTransform(mean, std ** 2, False),\n",
    "\t],\n",
    "\tsampler=test_sampler,\n",
    "\tworker_count=2,\n",
    ")\n",
    "test_steps_per_epoch = len(test_dataset) // batch_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We define a simple metrics aggregator below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Metric:\n",
    "\ttotal: float = 0.\n",
    "\tprevious: float = 0.\n",
    "\tcounter: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "\tdef __init__(self, metrics: list[str]) -> None:\n",
    "\t\tself.keys = metrics\n",
    "\t\tself.history = []\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self) -> None:\n",
    "\t\tif hasattr(self, 'metrics'):\n",
    "\t\t\tself.history.append(self.epoch_dict)\n",
    "\t\tself.metrics = {k: Metric() for k in self.keys}\n",
    "\n",
    "\tdef update(self, metrics: dict[str, float|int]) -> None:\n",
    "\t\tfor k, v in metrics.items():\n",
    "\t\t\tself.metrics[k].total += v\n",
    "\t\t\tself.metrics[k].previous = v\n",
    "\t\t\tself.metrics[k].counter += 1\n",
    "\n",
    "\tdef save(self, path: str) -> None:\n",
    "\t\twith open(path, 'w') as f:\n",
    "\t\t\tjson.dump(self.history, f)\n",
    "\n",
    "\t@property\n",
    "\tdef epoch_dict(self) -> dict[str, float]:\n",
    "\t\treturn {k: v.total / v.counter for k, v in self.metrics.items()}\n",
    "\n",
    "\t@property\n",
    "\tdef epoch(self) -> str:\n",
    "\t\treturn '\\t'.join([f'{k}: {v.total / v.counter:.4f}' for k, v in self.metrics.items()])\n",
    "\n",
    "\t@property\n",
    "\tdef previous(self) -> str:\n",
    "\t\treturn ', '.join([f'{k}: {v.previous:.4f}' for k, v in self.metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(preds: jnp.ndarray, labels: jnp.ndarray) -> float:\n",
    "\treturn (preds.argmax(axis=-1) == labels).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(metrics, loss, preds, labels):\n",
    "\taccuracy = calc_acc(preds, labels)\n",
    "\tmetrics.update({\n",
    "\t\t'loss': loss,\n",
    "\t\t'accuracy': accuracy,\n",
    "\t})\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Here, we implement a learnable QNN circuit. We use `dm_pix.extract_patches` to extract the patches of the image for the convolution. Then, we use `jax.vmap` to execute the QNN in a vectorized form, returning an array of shape `(batch_size, qnn_output_x * qnn_output_y * qnn_output_channels)`. We use a linear layer as a head for this model.\n",
    "\n",
    "We also define a CNN (LeNet-5) with a similar FLOPs requirement as the QNN to compare the effectiveness given the same amount of classical compute power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=qnn_wires)\n",
    "\n",
    "@partial(jax.jit, static_argnames=('wires',))\n",
    "@qml.qnode(dev) #let params have uh 10 qubits (we're going to cry)\n",
    "def learnable_qnn_circuit(param, phi, wires):\n",
    "\tfor wire in range(wires):\n",
    "\t\tqml.RY(np.pi * (param[0][wire][0] * phi[wire] + param[0][wire][1]), wires=wire)\n",
    "\t\tqml.RX(param[0][wire][2], wires=wire)\n",
    "\n",
    "\tfor layer_weights in param[2:]:\n",
    "\t\tfor wire in range(wires):\n",
    "\t\t\tqml.Rot(*layer_weights[wire], wires=wire)\n",
    "\t\tfor wire in range(wires):\n",
    "\t\t\tqml.CNOT(wires=[wire, (wire+1) % wires])\n",
    "\n",
    "\treturn qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "class LearnableQNN(nn.Module):\n",
    "\twires: int\n",
    "\tlayers: int\n",
    "\n",
    "\tdef setup(self):\n",
    "\t\tself.qnn_params = self.param('qnn_params', nn.initializers.uniform(scale=2*jnp.pi), (self.wires, self.layers, 3))\n",
    "\t\tself.kernel_width = int(self.wires**0.5)\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\tn = x.shape[0]\n",
    "\t\tpatches = dm_pix.extract_patches(\n",
    "\t\t\timages=x,\n",
    "\t\t\tsizes=(1, self.kernel_width, self.kernel_width, 1),\n",
    "\t\t\tstrides=(1, self.kernel_width, self.kernel_width, 1),\n",
    "\t\t\trates=(1, 1, 1, 1),\n",
    "\t\t\tpadding='VALID',\n",
    "\t\t)\n",
    "\t\tpatches = patches.reshape(-1, self.wires)\n",
    "\t\treturn jax.vmap(learnable_qnn_circuit, in_axes=(None, 0, None))(self.qnn_params, patches, self.wires).reshape(n, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLinearModel(nn.Module):\n",
    "\tnum_classes: int\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self, x: jnp.ndarray):\n",
    "\t\tx = nn.Dense(\n",
    "\t\t\tself.num_classes, name='head', kernel_init=nn.zeros\n",
    "\t\t)(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "\tlayers: list[nn.Module]\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self, x: jnp.ndarray):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tx = layer(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\tnum_classes: int\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self, x: jnp.ndarray):\n",
    "\t\tx = nn.Conv(features=6, kernel_size=(5, 5), strides=(1, 1), padding='VALID')(x)\n",
    "\t\tx = nn.relu(x)\n",
    "\t\tx = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n",
    "\n",
    "\t\tx = nn.Conv(features=16, kernel_size=(5, 5), strides=(1, 1), padding='VALID')(x)\n",
    "\t\tx = nn.relu(x)\n",
    "\t\tx = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n",
    "\n",
    "\t\tx = x.reshape((x.shape[0], -1))\n",
    "\t\tx = nn.Dense(120)(x)\n",
    "\t\tx = nn.relu(x)\n",
    "\t\tx = nn.Dense(84)(x)\n",
    "\t\tx = nn.relu(x)\n",
    "\t\tx = nn.Dense(self.num_classes)(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, rng, lr, n_epochs, train_steps_per_epoch, print_summary):\n",
    "\tvariables = module.init(rng, jnp.ones([1, 28, 28, 1]))\n",
    "\tparams = variables['params']\n",
    "\n",
    "\tlr_schedule = optax.cosine_onecycle_schedule(\n",
    "\t\ttransition_steps=n_epochs * train_steps_per_epoch,\n",
    "\t\tpeak_value=lr,\n",
    "\t\tpct_start=.1,\n",
    "\t\tfinal_div_factor=1000,\n",
    "\t)\n",
    "\tsolver = optax.yogi(lr_schedule)\n",
    "\n",
    "\tif print_summary:\n",
    "\t\tprint(module.tabulate(rng, jnp.ones((1, 28, 28, 1)), compute_flops=True, compute_vjp_flops=True))\n",
    "\n",
    "\treturn train_state.TrainState.create(\n",
    "\t\tapply_fn=module.apply,\n",
    "\t\tparams=params,\n",
    "\t\ttx=solver,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, images, labels):\n",
    "\tdef forward_and_loss(params, images, labels):\n",
    "\t\tpreds = state.apply_fn({ 'params': params }, images)\n",
    "\t\tloss = optax.losses.softmax_cross_entropy_with_integer_labels(\n",
    "\t\t\tlogits=preds, labels=labels\n",
    "\t\t).mean()\n",
    "\t\treturn loss, preds\n",
    "\n",
    "\t(loss, preds), grads = jax.value_and_grad(forward_and_loss, has_aux=True)(state.params, images, labels)\n",
    "\tstate = state.apply_gradients(grads=grads)\n",
    "\treturn state, loss, preds\n",
    "\n",
    "@jax.jit\n",
    "def test_step(state, images, labels):\n",
    "\tpreds = state.apply_fn({ 'params': state.params }, images)\n",
    "\tloss = optax.losses.softmax_cross_entropy_with_integer_labels(\n",
    "\t\tlogits=preds, labels=labels\n",
    "\t).mean()\n",
    "\treturn state, loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(epoch_type, state, train_loader, train_steps_per_epoch, metrics):\n",
    "\tassert epoch_type in ['Train', 'Test']\n",
    "\tfor images, labels in (pbar := tqdm(train_loader, total=train_steps_per_epoch, desc=epoch_type, leave=False)):\n",
    "\t\tif epoch_type == 'Train':\n",
    "\t\t\tstate, loss, preds = train_step(state, images, labels)\n",
    "\t\telse:\n",
    "\t\t\tstate, loss, preds = test_step(state, images, labels)\n",
    "\t\tupdate_metrics(metrics, loss, preds, labels)\n",
    "\t\tpbar.set_postfix_str(metrics.previous)\n",
    "\n",
    "\ttqdm.write(f'   -> {epoch_type}:\\t{metrics.epoch}')\n",
    "\tmetrics.reset()\n",
    "\treturn state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_train_metrics = Metrics(['loss', 'accuracy'])\n",
    "qnn_test_metrics = Metrics(['loss', 'accuracy'])\n",
    "cnn_train_metrics = Metrics(['loss', 'accuracy'])\n",
    "cnn_test_metrics = Metrics(['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, qnn_init_key, cnn_init_key = jax.random.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn_module = Sequential(layers=[LearnableQNN(wires=qnn_wires, layers=qnn_layers), BasicLinearModel(num_classes=10)])\n",
    "qnn_state = create_train_state(qnn_module, qnn_init_key, lr, n_epochs, train_steps_per_epoch, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_module = LeNet5(num_classes=10)\n",
    "cnn_state = create_train_state(cnn_module, cnn_init_key, lr, n_epochs, train_steps_per_epoch, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(1, n_epochs+1, desc='QNN'):\n",
    "\ttrain_sampler = train_sampler_fn(seed=epoch)\n",
    "\ttrain_loader = train_loader_fn(sampler=train_sampler)\n",
    "\n",
    "\ttqdm.write(f'Epoch {epoch}/{n_epochs}')\n",
    "\n",
    "\tqnn_state = run_epoch('Train', qnn_state, train_loader, train_steps_per_epoch, qnn_train_metrics)\n",
    "\tqnn_state = run_epoch('Test', qnn_state, test_loader, test_steps_per_epoch, qnn_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(1, n_epochs+1, desc='CNN'):\n",
    "\ttrain_sampler = train_sampler_fn(seed=epoch)\n",
    "\ttrain_loader = train_loader_fn(sampler=train_sampler)\n",
    "\n",
    "\ttqdm.write(f'Epoch {epoch}/{n_epochs}')\n",
    "\n",
    "\tcnn_state = run_epoch('Train', cnn_state, train_loader, train_steps_per_epoch, cnn_train_metrics)\n",
    "\tcnn_state = run_epoch('Test', cnn_state, test_loader, test_steps_per_epoch, cnn_test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
